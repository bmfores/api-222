---
title: "API 222 Problem Set 2"
subtitle: "Machine Learning and Big Data Analytics: Spring 2024"
author: "Your Name"
output: pdf_document
---

```{r, include=FALSE, message=FALSE, warning=FALSE}
# The needed libraries
library(tidyverse)
library(stargazer)
library(FNN)
library(kableExtra)
library(glmnet)

# Ask R to not present numbers using scientific notation
options(scipen = 999)
```

This problem set is worth 30 points in total. To get full credit, submit your code along with a write-up of your answers. This should either be done in R Markdown or Jupyter Notebook, submitted in one knitted PDF.

## Final Project Groups (0 pts)

Please join one of 30 final project groups that have been created on this [Canvas](https://canvas.harvard.edu/courses/132784/groups#tab-25429) page. Details about the final project can be found on this [Canvas](https://canvas.harvard.edu/courses/132784/files?preview=19519057) page. You just need to form your group by March 7. The main project milestones will be after the midterm. We recommend forming groups of 5 students. All students working together should join the same group. PhD students need to work individually (see details on Canvas). If you are a PhD student or are otherwise working alone, please form a group by yourself. Please email Jacob if you have questions about this assignment or the final project.

## Conceptual Questions (15 pts)

1.  Consider the four main classification methods that have been presented thus far this semester: logistic regression, k-Nearest Neighbors, linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA. Which of these methods may be appropriate if you know the decision boundary between the classes is linear? (3pts) **Logistic regression and LDA**

2.  Suppose you had the following data and you are using KNN Regression with Euclidean distance. Consider the prediction problem where you want to predict Y for the data point X1 = X2 = X3 = 0.

| X1  | X2  | X3  | Y   |
|-----|-----|-----|-----|
| 0   | 3.5 | 2   | 2   |
| 1   | 2.1 | 3   | 1   |
| 2   | 4.7 | 1   | 3   |
| 1   | 3.9 | 1   | 2   |
| 0   | 2.9 | 2   | 4   |
| 1   | 1.5 | 2   | 1   |
| 1   | 3.5 | 4   | 2   |

(a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0. (1pt)

```{r}
# Your code here
# make a dataframe of data
data <- data.frame(
  X1 = c(0, 1, 2, 1, 0, 1, 1),
  X2 = c(3.5, 2.1, 4.7, 3.9, 2.9, 1.5, 3.5),
  X3 = c(2, 3, 1, 1, 2, 2, 4),
  Y = c(2, 1, 3, 2, 4, 1, 2)
)

# Create a test point with all zeroes
test_point <- c(0, 0, 0)

# Calculate Euclidean distances
distances <- sqrt(rowSums((data[, 1:3] - test_point)^2))

# Output the distances
distances
```

(b) What is your prediction with K = 2? Why? (1pt)

```{r}
# Your code here

```

(c) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why? (1pt) In cases of highly nonlinear Bayes decision boundaries, a smaller K in the K-nearest neighbors (KNN) algorithm is preferred. This choice offers increased flexibility, aiding in capturing complex relationships within the data. However, caution is necessary to avoid overfitting, as small K values may lead to sensitivity to noise and outliers.

```{r}
# Your code here
```

3.  Consider we conduct a research study analyzing the risk factors for developing prostate cancer among men, with variables $X_1 = \text{age (years)}$, $X_2 = \text{family history of prostate cancer (0 = no, 1 = yes)}$, $X_3 = \text{smoking status (0 = non-smoker, 1 = smoker)}$, and $Y = \text{probability of developing prostate cancer}$. A logistic regression analysis is performed, resulting in estimated coefficients $\hat{\beta}_1 = 0.06$, $\hat{\beta}_2 = 1.2$, $\hat{\beta}_3 = 0.8$, and $\hat{\beta}_0 = -3.5$.

<!-- -->

(a) Interpret $\hat{\beta}_2$. (1 pt)\
    The estimated coefficient $\hat{\beta}_2$ - 1.2\$ for the variable X2, representing family history of prostate cancer, indicates that individuals with a family history of prostate cancer (coded as 1) have a 1.2 unit increase in the log-odds of developing prostate cancer compared to individuals without such a family history (coded as 0), holding other variables constant.

Having a family history of prostate cancer is associated with a 1.2 increase in the log-odds of developing prostate cancer, given that smoker status and age are held constant.

(b) Estimate the probability that a 60-year-old man with a family history of prostate cancer who is a smoker develops prostate cancer. (2 pts)

<!-- -->

4.  k-fold cross-validation

<!-- -->

(a) Briefly explain how k-fold cross-validation is implemented. (2pts)

K-fold cross-validation is implemented by first dividing the dataset into k equally sized folds. Then, the model is trained k times, each time using k-1 folds as the training set and the remaining fold as the validation set. For each iteration, the model's performance is evaluated using a chosen evaluation metric, such as accuracy or mean squared error, on the validation set. After all iterations are completed, the average performance metric across all folds is calculated to provide an estimate of the model's generalization performance. This process helps to assess the model's performance more accurately by reducing the variance that can arise from a single train-test split and ensures that each data point is used for both training and validation purposes.

(b) What are the advantages of k-fold cross-validation relative to the validation set approach? (1pt)

We're using the entire dataset. K-fold cross-validation offers advantages over the validation set approach by better utilizing data, reducing variance in performance estimates through averaging, and providing a more comprehensive evaluation across multiple data subsets. This approach mitigates sensitivity to data splits and biases, yielding more reliable assessments of model generalization.

5.  Suppose you want to minimize the false negative rate in your classification. You run two models: A and B. AUC for Model A is 0.7 and for Model B is 0.8. Can you conclude that you should choose Model B? Why or why not? (3 pts)

No. While Model B has a higher AUC (Area Under the ROC Curve) than Model A, choosing the best model solely based on AUC may not be appropriate if minimizing false negatives is the primary objective. It's crucial to consider the specific trade-offs between false negatives and false positives. Depending on the application, Model A might still be preferred if it achieves a more favorable balance between false negatives and false positives, even though it has a lower AUC than Model B.

## Applied Questions (15 pts)

**Predicting Hospital Length of Stay**

[Download data here.](https://canvas.harvard.edu/courses/132784/files/folder/Problem%20Set%20Data/Problem%20Set%202)

For the next portion of this assignment you will be working with the `LengthOfStay.csv` dataset. This dataset has data points on patients admitted into hospital, indicators of their health condition and how long they were admitted in the hospital.

This is an important problem in healthcare. In order for hospitals to optimize resource allocation, it is important to predict accurately how long a newly admitted patient will stay in the hospital.

1.  What are the dimensions of the dataset? (1 pt)

2.  Use the `cor()` function to display the correlations of all **continuous** variables in the dataset. Which variables is most highly correlated with `lengthofstay`? (2 pts)

Consider the prediction problem where you want to predict the length of stay for a patient (`lengthofstay`) against all other variables available in the data set.

3.  Run ridge regression with cross-validation and standardized features using the canned function `cv.glmnet` from the package `glmnet`. You can use the $\lambda$ sequence generated by `cv.glment` (you do not need to provide your own $\lambda$ sequence). In order to receive credit for this question, make the line immediately preceding this command say `set.seed(222)` and run the two lines together. Please report all numbers by rounding to three decimal places. (2 pts)

<!-- -->

(a) Which $\lambda$ had the lowest mean cross-validation error? (1 pt)

(b) What was the cross-validation error? (1 pt)

(c) What was the standard error of the mean cross-validation error for this value of$\lambda$? (1 pt)

(d) What was the largest value of $\lambda$ whose mean cross validation error was within one standard deviation of the lowest cross-validation error? (1 pt)

<!-- -->

5.  Now consider the same prediction problem. Implement your own 5-fold cross-validation routine for KNN for $K = 1, ..., 50$ (write the cross-validation routine yourself rather than using a canned package). Include the snippet of code you wrote here. It should not exceed 20 lines. (6pts)

<!-- -->

(a) Plot of mean cross-validation MSE as a function of $k$.

(b) The best k according to CV is

(c) The cross-validation error for the best k is
